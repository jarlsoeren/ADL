{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c0abd51",
      "metadata": {
        "id": "2c0abd51"
      },
      "source": [
        "# Feedforward classification of the NMIST data\n",
        "### Advanced Deep Learning 2024\n",
        "This notebook was written originally Jon Sporring (mailto:sporring@di.ku.dk) and heavily inspired by https://clay-atlas.com/us/blog/2021/04/22/pytorch-en-tutorial-4-train-a-model-to-classify-mnist.\n",
        "\n",
        "We consider the Modified National Institute of Standards and Technology database of handwritten digits (MNIST): http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b1cd80",
      "metadata": {
        "id": "a4b1cd80"
      },
      "source": [
        "## Installs\n",
        "\n",
        "On non-colab system, is usually good to make an environment and install necessary tools there. E.g., anaconda->jupyter->terminal create an environment, if you have not already, and activate it:\n",
        "```\n",
        "conda create -n adl python=3.9\n",
        "conda activate adl\n",
        "```\n",
        "then install missing packages such as:\n",
        "```\n",
        "conda install ipykernel torch matplotlib torchmetrics scikit-image jpeg\n",
        "conda install -c conda-forge segmentation-models-pytorch ipywidgets\n",
        "```\n",
        "and if you want to add it to jupyter's drop-down menu\n",
        "```\n",
        "ipython kernel install --user --name=adl\n",
        "```\n",
        "Now reload the jupyter-notebook's homepage and make a new or load an existing file. On colab, the tools have to be installed everytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4afe90fa",
      "metadata": {
        "id": "4afe90fa"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0112f199",
      "metadata": {
        "id": "0112f199"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "372f3da4",
      "metadata": {
        "id": "372f3da4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as dset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709fa153",
      "metadata": {
        "id": "709fa153"
      },
      "source": [
        "## Set global device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef384f3a",
      "metadata": {
        "id": "ef384f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU State: mps\n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print('GPU State:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0479fa7",
      "metadata": {
        "id": "c0479fa7"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "16d728ef",
      "metadata": {
        "id": "16d728ef"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, loss, optimizer, loader, epochs, verbose=True, device=device):\n",
        "    \"\"\"\n",
        "    Run training of a model given a loss function, optimizer and a set of training and validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for times, data in enumerate(loader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Foward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss_tensor = loss(outputs, labels)\n",
        "            loss_tensor.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss_tensor.item()\n",
        "            if verbose:\n",
        "                if times % 100 == 99 or times+1 == len(loader):\n",
        "                    print('[%d/%d, %d/%d] loss: %.3f' % (epoch+1, epochs, times+1, len(loader), running_loss/2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d7ce08f",
      "metadata": {
        "id": "2d7ce08f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader, device=device):\n",
        "    \"\"\"\n",
        "    Evaluate a model 'model' on all batches of a torch DataLoader 'data_loader'.\n",
        "\n",
        "    Returns: the total number of correct classifications,\n",
        "             the total number of images\n",
        "             the list of the per class correct classification,\n",
        "             the list of the per class total number of images.\n",
        "    \"\"\"\n",
        "\n",
        "    # Test\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    class_correct = [0 for i in range(10)]\n",
        "    class_total = [0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            inputs = inputs.view(inputs.shape[0], -1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(10):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    return (correct, total, class_correct, class_total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd353eee",
      "metadata": {
        "id": "cd353eee"
      },
      "source": [
        "## Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "142ae598",
      "metadata": {
        "id": "142ae598"
      },
      "outputs": [],
      "source": [
        "# Transform\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,)),]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "94591bc3",
      "metadata": {
        "id": "94591bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "trainSet = datasets.MNIST(root='MNIST', download=True, train=True, transform=transform)\n",
        "testSet = datasets.MNIST(root='MNIST', download=True, train=False, transform=transform)\n",
        "trainLoader = dset.DataLoader(trainSet, batch_size=64, shuffle=True)\n",
        "testLoader = dset.DataLoader(testSet, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "848e7dcc",
      "metadata": {
        "id": "848e7dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (5): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=784, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=64, out_features=10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fce0735b",
      "metadata": {
        "id": "fce0735b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 60000 images\n",
            "[1/4, 100/938] loss: 0.108\n",
            "[1/4, 200/938] loss: 0.180\n",
            "[1/4, 300/938] loss: 0.218\n",
            "[1/4, 400/938] loss: 0.246\n",
            "[1/4, 500/938] loss: 0.270\n",
            "[1/4, 600/938] loss: 0.291\n",
            "[1/4, 700/938] loss: 0.311\n",
            "[1/4, 800/938] loss: 0.330\n",
            "[1/4, 900/938] loss: 0.349\n",
            "[1/4, 938/938] loss: 0.355\n",
            "[2/4, 100/938] loss: 0.017\n",
            "[2/4, 200/938] loss: 0.034\n",
            "[2/4, 300/938] loss: 0.050\n",
            "[2/4, 400/938] loss: 0.067\n",
            "[2/4, 500/938] loss: 0.084\n",
            "[2/4, 600/938] loss: 0.098\n",
            "[2/4, 700/938] loss: 0.113\n",
            "[2/4, 800/938] loss: 0.128\n",
            "[2/4, 900/938] loss: 0.143\n",
            "[2/4, 938/938] loss: 0.148\n",
            "[3/4, 100/938] loss: 0.014\n",
            "[3/4, 200/938] loss: 0.028\n",
            "[3/4, 300/938] loss: 0.041\n",
            "[3/4, 400/938] loss: 0.055\n",
            "[3/4, 500/938] loss: 0.068\n",
            "[3/4, 600/938] loss: 0.081\n",
            "[3/4, 700/938] loss: 0.092\n",
            "[3/4, 800/938] loss: 0.105\n",
            "[3/4, 900/938] loss: 0.117\n",
            "[3/4, 938/938] loss: 0.121\n",
            "[4/4, 100/938] loss: 0.012\n",
            "[4/4, 200/938] loss: 0.023\n",
            "[4/4, 300/938] loss: 0.034\n",
            "[4/4, 400/938] loss: 0.045\n",
            "[4/4, 500/938] loss: 0.055\n",
            "[4/4, 600/938] loss: 0.066\n",
            "[4/4, 700/938] loss: 0.077\n",
            "[4/4, 800/938] loss: 0.088\n",
            "[4/4, 900/938] loss: 0.098\n",
            "[4/4, 938/938] loss: 0.102\n",
            "Training Finished.\n",
            "\n",
            "Accuracy of the network on the 10000 test images: 94 %\n",
            "Accuracy of 0: 0.967105\n",
            "Accuracy of 1: 0.978378\n",
            "Accuracy of 2: 0.930233\n",
            "Accuracy of 3: 0.916667\n",
            "Accuracy of 4: 0.949153\n",
            "Accuracy of 5: 0.904762\n",
            "Accuracy of 6: 0.929688\n",
            "Accuracy of 7: 0.945122\n",
            "Accuracy of 8: 0.944056\n",
            "Accuracy of 9: 0.940120\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "epochs = 4\n",
        "lr = 0.002\n",
        "loss = nn.NLLLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9)\n",
        "\n",
        "# Train\n",
        "print('Training on %d images' % trainSet.data.shape[0])\n",
        "training_loop(net, loss, optimizer, trainLoader, epochs)\n",
        "print('Training Finished.\\n')\n",
        "\n",
        "# Test\n",
        "correct, total, class_correct, class_total = evaluate_model(net, testLoader)\n",
        "print('Accuracy of the network on the %d test images: %d %%' % (testSet.data.shape[0], (100*correct / total)))\n",
        "for i in range(10):\n",
        "    print('Accuracy of %d: %3f' % (i, (class_correct[i]/class_total[i])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f06850a",
      "metadata": {
        "id": "7f06850a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "adl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
